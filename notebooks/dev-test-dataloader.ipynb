{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b416e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from bert.data import prepare_pretraining_dataset, TrainingCollator\n",
    "from transformers import BertTokenizer, BertTokenizerFast, default_data_collator, DataCollatorForWholeWordMask\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08c7187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "35715f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Optional, Any, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class DataCollatorForWholeWordMaskDeterministic(DataCollatorForWholeWordMask):\n",
    "    def __init__(self, *args, random_seed: int = 0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.random_seed = random_seed\n",
    "        self.call_counter = 0\n",
    "    \n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        random.seed(self.random_seed + self.call_counter)\n",
    "        self.call_counter += 1\n",
    "        return super().__call__(features, return_tensors)\n",
    "\n",
    "\n",
    "    def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n",
    "        'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n",
    "                \" --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "\n",
    "        probability_matrix = mask_labels\n",
    "\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        if self.tokenizer.pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "\n",
    "        masked_indices = probability_matrix.bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "        inputs[masked_indices] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc3daf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fast = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer_slow = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6923eee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7664867f8e3f49838fade1acbb1606ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = prepare_pretraining_dataset(tokenizer_fast, sample_limit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b9e66c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches():\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset[\"test\"],\n",
    "        batch_size=5,\n",
    "        shuffle = False,\n",
    "        num_workers = 0,\n",
    "        collate_fn = DataCollatorForWholeWordMaskDeterministic(\n",
    "                tokenizer_slow, mlm=True, mlm_probability=0.15,\n",
    "                return_tensors=\"pt\", random_seed = 0\n",
    "            )\n",
    "    )\n",
    "    iterator = iter(dataloader)\n",
    "    batches = [next(iterator) for _ in range(5)]\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a770e0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batches1 = get_batches()\n",
    "print()\n",
    "batches2 = get_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "505c1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b1, b2 in zip(batches1, batches2):\n",
    "    assert b1[\"input_ids\"].count_nonzero() == b2[\"input_ids\"].count_nonzero()\n",
    "    assert torch.equal(b1[\"input_ids\"], b2[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b1d170f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset[\"test\"],\n",
    "    batch_size=5,\n",
    "    shuffle = False,\n",
    "    num_workers = 0,\n",
    "    collate_fn = DataCollatorForWholeWordMaskDeterministic(\n",
    "            tokenizer_slow, mlm=True, mlm_probability=0.15,\n",
    "            return_tensors=\"pt\", random_seed = 0\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "74a6e2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForWholeWordMaskDeterministic(tokenizer=BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), mlm=True, mlm_probability=0.15, mask_replace_prob=0.8, random_replace_prob=0.1, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6503752b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.collate_fn.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410c724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-from-scratch]",
   "language": "python",
   "name": "conda-env-pytorch-from-scratch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
