{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5ddc92",
   "metadata": {},
   "source": [
    "# Following the code from here\n",
    "https://huggingface.co/learn/nlp-course/en/chapter6/8?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b78a5f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d73b3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b99e6e4c8f2407ab681d8426cde5999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])  # only keep the 'text' column\n",
    " \n",
    "assert bookcorpus.features.type == wiki.features.type\n",
    "raw_datasets = concatenate_datasets([bookcorpus, wiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af3d57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(raw_datasets), 1000):\n",
    "        yield raw_datasets[i : i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26bdb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7550e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[], normalizer=None, pre_tokenizer=None, post_processor=None, decoder=None, model=WordPiece(unk_token=\"[UNK]\", continuing_subword_prefix=\"##\", max_input_chars_per_word=100, vocab={}))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be89de",
   "metadata": {},
   "source": [
    "## 1. Normalization (any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e3a04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(\n",
    "    clean_text = True, # remove all control characters and repleace repeating strings with a single one\n",
    "    handle_chinese_chars=True, # place spaces around Chinese characters\n",
    "    strip_accents=None, # whether to strip accents\n",
    "    lowercase=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b014ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accents should be stripped since we are building an \"uncased\" tokenizer\n",
    "# NFD = unicode normalizer\n",
    "# shoudl also include 2x normalizers.Repalce Regex replacmeents\n",
    "# tokenizer.normalizer = normalizers.Sequence(\n",
    "#     [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c51f9dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.normalizer.normalize_str(\"Héllò\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87f07d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 你  好 '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.normalizer.normalize_str(\"你好\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272456b3",
   "metadata": {},
   "source": [
    "## 2. Pre-tokenization (splitting the input into words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f3ea0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits on whitespace and any character that isn't a letter, digit, or underscore\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "# alternatively tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0af775b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f4d79c",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "Run the input through the model (using the pre-tokenized words to produce a sequence of tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d09b440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=30522, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd629bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f341b337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'test', 'this', 'to', '##ke', '##n', '##ize', '##r', '.']\n"
     ]
    }
   ],
   "source": [
    "# generate encoding. encoding contains fields\n",
    "# ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, and overflowing\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb53a49d",
   "metadata": {},
   "source": [
    "## 4. Post-processing\n",
    "(adding the special tokens of the tokenizer, generating the attention mask and token type IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e4e9525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a91f5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cfbf7252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'to', '##ke', '##n', '##ize', '##r', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "89243faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'to', '##ke', '##n', '##ize', '##r', '.', '.', '.', '[SEP]', 'on', 'a', 'pair', 'of', 'sent', '##ences', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c76f7a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids:  [2, 26173, 11, 61, 26723, 25217, 25052, 27426, 19872, 26155, 19861, 18, 18, 18, 3, 25072, 43, 29235, 25044, 26707, 28536, 18, 3]\n",
      "offsets:  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"ids: \", encoding.ids)\n",
    "print(\"special token mask: \", encoding.special_tokens_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fca1d3",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e968a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9705162b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"let ' s test this tokenizer... on a pair of sentences.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e1450b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ac0d1",
   "metadata": {},
   "source": [
    "## Fast Tokenizer wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21d4da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "# from transformers import BertTokenizerFast\n",
    "# wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-from-scratch]",
   "language": "python",
   "name": "conda-env-pytorch-from-scratch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
