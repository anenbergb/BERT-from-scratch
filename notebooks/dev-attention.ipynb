{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8529022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import BertTokenizerFast, BertTokenizer\n",
    "import functools\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator, DataCollatorForWholeWordMask\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bert.model import BertConfig,BertMLM\n",
    "from bert.train import TrainingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e670d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e776a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_tokenize_and_chunk(tokenizer, max_length=32):\n",
    "    def tokenize_and_chunk(examples):\n",
    "        # https://huggingface.co/docs/transformers/main/en/pad_truncation\n",
    "        result = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation = True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_overflowing_tokens=True,\n",
    "        )\n",
    "        result.pop(\"overflow_to_sample_mapping\")\n",
    "        return result\n",
    "    return tokenize_and_chunk\n",
    "\n",
    "#         if tokenizer.is_fast:\n",
    "#             # word_ids maps each token to the index of the word in the source sentence that it came from\n",
    "#             result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "\n",
    "        # Create a new labels column\n",
    "#         result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "#         # Extract mapping between new and old indices\n",
    "#         sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "#         for key, values in examples.items():\n",
    "#             result[key] = [values[i] for i in sample_map]\n",
    "        \n",
    "#         import ipdb\n",
    "#         ipdb.set_trace()\n",
    "#         return result\n",
    "\n",
    "# To get the normalized and pre-tokenized list of words that \"word_ids\" will index into\n",
    "def pretokenizer(tokenizer, text):\n",
    "    normalizer = tokenizer.backend_tokenizer.normalizer\n",
    "    pretokenizer = tokenizer.backend_tokenizer.pre_tokenizer\n",
    "    return pretokenizer.pre_tokenize_str(normalizer.normalize_str(text))\n",
    "\n",
    "## Given input_ids, can convert those to string tokens, e.g. ##word\n",
    "# tokenizer.convert_ids_to_tokens(tokenized_dataset[index][\"input_ids\"])\n",
    "## Given input_ids, can decode those to the original text (e.g. concat tokens)\n",
    "# tokenizer.decode(tokenized_dataset[index][\"input_ids\"])\n",
    "## Special tokens\n",
    "# tokenizer.all_special_tokens = ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
    "# tokenizer.all_special_ids = [100, 102, 0, 101, 103]\n",
    "\n",
    "\n",
    "class TrainingCollator:\n",
    "    def __init__(self, tokenizer, train_config):\n",
    "        self.pass_through_keys = [\"token_type_ids\", \"attention_mask\"]\n",
    "        self.collator = DataCollatorForWholeWordMask(\n",
    "            tokenizer,\n",
    "            mlm = True,\n",
    "            mlm_probability=train_config.mask_lm_prob,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "    def __call__(self, examples):\n",
    "        pass_through_examples = []\n",
    "        input_ids = []\n",
    "        for example in examples:\n",
    "            pass_through = {\n",
    "                key: example[key] for key in self.pass_through_keys\n",
    "            }\n",
    "            pass_through[\"original_input_ids\"] = example[\"input_ids\"].copy()\n",
    "            pass_through_examples.append(pass_through)\n",
    "            input_ids.append({\n",
    "                \"input_ids\": example[\"input_ids\"]\n",
    "            })\n",
    "        \n",
    "        batch = {**default_data_collator(pass_through_examples, return_tensors=\"pt\"),\n",
    "                 **self.collator(examples)\n",
    "                }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4790921",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig()\n",
    "train_config = TrainingConfig(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea72e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bookcorpus\", split=\"train\").select(range(100000,200000))\n",
    "# dataset = dataset.map(lambda samples: {\"text_length\": [len(text) for text in samples[\"text\"]]}, batched=True)\n",
    "# dataset = dataset.sort(\"text_length\", reverse=True)\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07ba0b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65d81f4b6c2477084f95cb6e7339626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 100000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "assert tokenizer.vocab_size == config.vocab_size\n",
    "tokenized_dataset = dataset.map(wrap_tokenize_and_chunk(tokenizer, train_config.initial_sequence_length), batched=True, remove_columns=dataset.column_names)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec4ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=TrainingCollator(tokenizer, train_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe58548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceae7f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertMLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b990e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token_type_ids', 'attention_mask', 'original_input_ids', 'input_ids', 'labels'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a45e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "873990ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 30522])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d38705b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-from-scratch]",
   "language": "python",
   "name": "conda-env-pytorch-from-scratch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
