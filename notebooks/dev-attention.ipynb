{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8529022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "import functools\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator, DataCollatorForWholeWordMask\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bert.model import BertConfig, BertEmbeddings\n",
    "from bert.train import TrainingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e670d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e776a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "def wrap_tokenize_and_chunk(tokenizer, max_length=32):\n",
    "    def tokenize_and_chunk(examples):\n",
    "        # https://huggingface.co/docs/transformers/main/en/pad_truncation\n",
    "        result = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation = True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_overflowing_tokens=True,\n",
    "        )\n",
    "        result.pop(\"overflow_to_sample_mapping\")\n",
    "        return result\n",
    "    return tokenize_and_chunk\n",
    "\n",
    "#         if tokenizer.is_fast:\n",
    "#             # word_ids maps each token to the index of the word in the source sentence that it came from\n",
    "#             result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "\n",
    "        # Create a new labels column\n",
    "#         result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "#         # Extract mapping between new and old indices\n",
    "#         sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "#         for key, values in examples.items():\n",
    "#             result[key] = [values[i] for i in sample_map]\n",
    "        \n",
    "#         import ipdb\n",
    "#         ipdb.set_trace()\n",
    "#         return result\n",
    "\n",
    "# To get the normalized and pre-tokenized list of words that \"word_ids\" will index into\n",
    "def pretokenizer(tokenizer, text):\n",
    "    normalizer = tokenizer.backend_tokenizer.normalizer\n",
    "    pretokenizer = tokenizer.backend_tokenizer.pre_tokenizer\n",
    "    return pretokenizer.pre_tokenize_str(normalizer.normalize_str(text))\n",
    "\n",
    "## Given input_ids, can convert those to string tokens, e.g. ##word\n",
    "# tokenizer.convert_ids_to_tokens(tokenized_dataset[index][\"input_ids\"])\n",
    "## Given input_ids, can decode those to the original text (e.g. concat tokens)\n",
    "# tokenizer.decode(tokenized_dataset[index][\"input_ids\"])\n",
    "## Special tokens\n",
    "# tokenizer.all_special_tokens = ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
    "# tokenizer.all_special_ids = [100, 102, 0, 101, 103]\n",
    "\n",
    "\n",
    "class TrainingCollator:\n",
    "    def __init__(self, tokenizer, train_config):\n",
    "        self.pass_through_keys = [\"token_type_ids\", \"attention_mask\"]\n",
    "        self.collator = DataCollatorForWholeWordMask(\n",
    "            tokenizer,\n",
    "            mlm = True,\n",
    "            mlm_probability=train_config.mask_lm_prob,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "    def __call__(self, examples):\n",
    "        pass_through_examples = []\n",
    "        input_ids = []\n",
    "        for example in examples:\n",
    "            pass_through = {\n",
    "                key: example[key] for key in self.pass_through_keys\n",
    "            }\n",
    "            pass_through[\"original_input_ids\"] = example[\"input_ids\"].copy()\n",
    "            pass_through_examples.append(pass_through)\n",
    "            input_ids.append({\n",
    "                \"input_ids\": example[\"input_ids\"]\n",
    "            })\n",
    "        \n",
    "        batch = {**default_data_collator(pass_through_examples, return_tensors=\"pt\"),\n",
    "                 **self.collator(examples)\n",
    "                }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e4790921",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig()\n",
    "train_config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dea72e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bookcorpus\", split=\"train\").select(range(100000,200000))\n",
    "# dataset = dataset.map(lambda samples: {\"text_length\": [len(text) for text in samples[\"text\"]]}, batched=True)\n",
    "# dataset = dataset.sort(\"text_length\", reverse=True)\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "07ba0b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 100000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "assert tokenizer.vocab_size == config.vocab_size\n",
    "tokenized_dataset = dataset.map(wrap_tokenize_and_chunk(tokenizer, train_config.initial_sequence_length), batched=True, remove_columns=dataset.column_names)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bec4ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=train_config.batch_size,\n",
    "    collate_fn=TrainingCollator(tokenizer, train_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "49b42122",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ceae7f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings = BertEmbeddings(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5d06ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bert_embeddings(batch[\"input_ids\"], batch[\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3537395c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 768])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "05c6c384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 768])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "df7433fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2002,  9826,  ...,     0,     0,     0],\n",
       "        [  101,  2002,   103,  ...,     0,     0,     0],\n",
       "        [  101,  2009,  2001,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  1036,   103,  ...,     0,     0,     0],\n",
       "        [  101, 10882,  2721,  ...,     0,     0,     0],\n",
       "        [  101, 10021,  4841,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7c72e3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3491e+00,  1.2063e-01,  9.9072e-01,  ...,  3.0361e+00,\n",
       "          -1.9730e-01, -6.8039e-01],\n",
       "         [ 1.3265e-01, -1.0415e-01,  5.6475e-01,  ..., -3.9019e-01,\n",
       "           1.1565e+00,  1.3714e+00],\n",
       "         [-1.6426e+00, -1.5759e+00,  1.1122e+00,  ...,  1.4033e+00,\n",
       "          -3.3764e-01, -2.3891e-01],\n",
       "         ...,\n",
       "         [ 8.7195e-01, -8.6577e-01,  8.3622e-01,  ..., -5.8911e-02,\n",
       "           5.4693e-01, -5.4651e-01],\n",
       "         [-4.0395e-01,  1.6512e-01, -2.9788e-01,  ...,  6.4924e-01,\n",
       "           4.7402e-02, -1.7359e+00],\n",
       "         [-1.1760e+00,  8.9313e-02, -4.4666e-01,  ..., -1.0645e+00,\n",
       "           1.2450e+00,  5.3799e-01]],\n",
       "\n",
       "        [[-1.3491e+00,  1.2063e-01,  9.9072e-01,  ...,  3.0361e+00,\n",
       "          -1.9730e-01, -6.8039e-01],\n",
       "         [ 1.3265e-01, -1.0415e-01,  5.6475e-01,  ..., -3.9019e-01,\n",
       "           1.1565e+00,  1.3714e+00],\n",
       "         [-1.3423e+00, -1.6372e-01, -1.8336e-01,  ...,  4.3507e-01,\n",
       "           5.5937e-01, -1.0800e+00],\n",
       "         ...,\n",
       "         [ 8.7195e-01, -8.6577e-01,  8.3622e-01,  ..., -5.8911e-02,\n",
       "           5.4693e-01, -5.4651e-01],\n",
       "         [-4.0395e-01,  1.6512e-01, -2.9788e-01,  ...,  6.4924e-01,\n",
       "           4.7402e-02, -1.7359e+00],\n",
       "         [-1.1760e+00,  8.9313e-02, -4.4666e-01,  ..., -1.0645e+00,\n",
       "           1.2450e+00,  5.3799e-01]],\n",
       "\n",
       "        [[-1.3491e+00,  1.2063e-01,  9.9072e-01,  ...,  3.0361e+00,\n",
       "          -1.9730e-01, -6.8039e-01],\n",
       "         [ 2.0710e+00, -1.4751e+00,  2.1842e-01,  ...,  1.4047e-01,\n",
       "           1.8232e+00, -7.4231e-02],\n",
       "         [-1.2965e+00, -7.9805e-01,  2.5607e+00,  ...,  1.9379e+00,\n",
       "           1.4163e+00, -1.9321e+00],\n",
       "         ...,\n",
       "         [ 8.7195e-01, -8.6577e-01,  8.3622e-01,  ..., -5.8911e-02,\n",
       "           5.4693e-01, -5.4651e-01],\n",
       "         [-4.0395e-01,  1.6512e-01, -2.9788e-01,  ...,  6.4924e-01,\n",
       "           4.7402e-02, -1.7359e+00],\n",
       "         [-1.1760e+00,  8.9313e-02, -4.4666e-01,  ..., -1.0645e+00,\n",
       "           1.2450e+00,  5.3799e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.3491e+00,  1.2063e-01,  9.9072e-01,  ...,  3.0361e+00,\n",
       "          -1.9730e-01, -6.8039e-01],\n",
       "         [ 1.4064e+00, -8.7695e-02,  2.0976e+00,  ...,  8.1535e-01,\n",
       "          -4.9567e-01,  8.0501e-01],\n",
       "         [-1.3423e+00, -1.6372e-01, -1.8336e-01,  ...,  4.3507e-01,\n",
       "           5.5937e-01, -1.0800e+00],\n",
       "         ...,\n",
       "         [ 8.7195e-01, -8.6577e-01,  8.3622e-01,  ..., -5.8911e-02,\n",
       "           5.4693e-01, -5.4651e-01],\n",
       "         [-4.0395e-01,  1.6512e-01, -2.9788e-01,  ...,  6.4924e-01,\n",
       "           4.7402e-02, -1.7359e+00],\n",
       "         [-1.1760e+00,  8.9313e-02, -4.4666e-01,  ..., -1.0645e+00,\n",
       "           1.2450e+00,  5.3799e-01]],\n",
       "\n",
       "        [[-1.3491e+00,  1.2063e-01,  9.9072e-01,  ...,  3.0361e+00,\n",
       "          -1.9730e-01, -6.8039e-01],\n",
       "         [-6.8193e-02,  5.0941e-01,  9.8075e-01,  ...,  1.7628e+00,\n",
       "           1.5947e+00, -2.1379e+00],\n",
       "         [-2.6830e+00, -3.1570e-01,  1.7058e-03,  ...,  1.3504e+00,\n",
       "          -7.9580e-01,  7.2147e-01],\n",
       "         ...,\n",
       "         [ 8.7195e-01, -8.6577e-01,  8.3622e-01,  ..., -5.8911e-02,\n",
       "           5.4693e-01, -5.4651e-01],\n",
       "         [-4.0395e-01,  1.6512e-01, -2.9788e-01,  ...,  6.4924e-01,\n",
       "           4.7402e-02, -1.7359e+00],\n",
       "         [-1.1760e+00,  8.9313e-02, -4.4666e-01,  ..., -1.0645e+00,\n",
       "           1.2450e+00,  5.3799e-01]],\n",
       "\n",
       "        [[-1.3491e+00,  1.2063e-01,  9.9072e-01,  ...,  3.0361e+00,\n",
       "          -1.9730e-01, -6.8039e-01],\n",
       "         [-4.2576e-02, -1.5304e+00,  1.4128e+00,  ..., -2.2173e-01,\n",
       "          -2.3213e-01,  6.5976e-01],\n",
       "         [-4.7752e-01,  6.3557e-01, -6.7318e-01,  ...,  3.6035e-01,\n",
       "           2.1394e+00, -2.0128e+00],\n",
       "         ...,\n",
       "         [ 8.7195e-01, -8.6577e-01,  8.3622e-01,  ..., -5.8911e-02,\n",
       "           5.4693e-01, -5.4651e-01],\n",
       "         [-4.0395e-01,  1.6512e-01, -2.9788e-01,  ...,  6.4924e-01,\n",
       "           4.7402e-02, -1.7359e+00],\n",
       "         [-1.1760e+00,  8.9313e-02, -4.4666e-01,  ..., -1.0645e+00,\n",
       "           1.2450e+00,  5.3799e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb + word_emb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-from-scratch]",
   "language": "python",
   "name": "conda-env-pytorch-from-scratch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
