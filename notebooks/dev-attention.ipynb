{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8901d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5913075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "def tokenize_function(tokenizer, examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        # word_ids maps each token to the index of the word in the source sentence that it came frome\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    \n",
    "    result[\"num_tokens\"] = [len(tokens) for tokens in result[\"input_ids\"]]\n",
    "    return result\n",
    "\n",
    "\n",
    "def tokenize_and_chunk(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation = True,\n",
    "        max_length=23,\n",
    "        padding=True,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    if tokenizer.is_fast:\n",
    "        # word_ids maps each token to the index of the word in the source sentence that it came frome\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    \n",
    "    # Extract mapping between new and old indices\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "        \n",
    "        \n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ba371eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bookcorpus\", split=\"train\").select(range(100000,200000))\n",
    "dataset = dataset.map(lambda samples: {\"text_length\": [len(text) for text in samples[\"text\"]]}, batched=True)\n",
    "dataset = dataset.sort(\"text_length\", reverse=True)\n",
    "\n",
    "#tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "#tokenized_dataset = dataset.map(functools.partial(tokenize_function, tokenizer), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f8e4c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_chunk(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation = True,\n",
    "        max_length=32,\n",
    "        padding=True,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    # Extract mapping between new and old indices\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "    \n",
    "    \n",
    "result = tokenize_and_chunk(dataset[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "66c7863e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"they were also generous with their wealth , but that was all done secretly , making sure that the people who had taken care of them as kids were now well cared for , including the cop who had released dominic for stealing some bread one day , or the teacher who had smacked the back of zayn 's head when he was n't concentrating on his math , or the neighborhood lady who would sit on her front stoop knitting hats for each of them so they were a little warmer on those freezing cold , winter days .\",\n",
       " \"third , that girl is desperate to talk about her business with someone , and since she 's coming to me that tells me that either you 're not her fiance and she does n't want to talk to you about anything , or you are her fiance but you have no business sense , which is probably the case because you have n't put a ring on her finger , or you 're a complete fuck-up who does n't care about the thing that 's most important to his future wife .\"]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:2][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c124f73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2027, 2020, 2036, 12382, 2007, 2037, 7177, 1010, 2021, 2008, 2001, 2035, 2589, 10082, 1010, 2437, 2469, 2008, 1996, 2111, 2040, 2018, 2579, 2729, 1997, 2068, 2004, 4268, 2020, 2085, 102], [101, 2092, 8725, 2005, 1010, 2164, 1996, 8872, 2040, 2018, 2207, 11282, 2005, 11065, 2070, 7852, 2028, 2154, 1010, 2030, 1996, 3836, 2040, 2018, 19203, 1996, 2067, 1997, 23564, 6038, 1005, 102], [101, 1055, 2132, 2043, 2002, 2001, 1050, 1005, 1056, 16966, 2006, 2010, 8785, 1010, 2030, 1996, 5101, 3203, 2040, 2052, 4133, 2006, 2014, 2392, 2358, 18589, 26098, 16717, 2005, 2169, 1997, 102], [101, 2068, 2061, 2027, 2020, 1037, 2210, 16676, 2006, 2216, 12809, 3147, 1010, 3467, 2420, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2353, 1010, 2008, 2611, 2003, 7143, 2000, 2831, 2055, 2014, 2449, 2007, 2619, 1010, 1998, 2144, 2016, 1005, 1055, 2746, 2000, 2033, 2008, 4136, 2033, 2008, 2593, 2017, 1005, 2128, 102], [101, 2025, 2014, 19154, 1998, 2016, 2515, 1050, 1005, 1056, 2215, 2000, 2831, 2000, 2017, 2055, 2505, 1010, 2030, 2017, 2024, 2014, 19154, 2021, 2017, 2031, 2053, 2449, 3168, 1010, 2029, 102], [101, 2003, 2763, 1996, 2553, 2138, 2017, 2031, 1050, 1005, 1056, 2404, 1037, 3614, 2006, 2014, 4344, 1010, 2030, 2017, 1005, 2128, 1037, 3143, 6616, 1011, 2039, 2040, 2515, 1050, 1005, 102], [101, 1056, 2729, 2055, 1996, 2518, 2008, 1005, 1055, 2087, 2590, 2000, 2010, 2925, 2564, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'text': [\"they were also generous with their wealth , but that was all done secretly , making sure that the people who had taken care of them as kids were now well cared for , including the cop who had released dominic for stealing some bread one day , or the teacher who had smacked the back of zayn 's head when he was n't concentrating on his math , or the neighborhood lady who would sit on her front stoop knitting hats for each of them so they were a little warmer on those freezing cold , winter days .\", \"they were also generous with their wealth , but that was all done secretly , making sure that the people who had taken care of them as kids were now well cared for , including the cop who had released dominic for stealing some bread one day , or the teacher who had smacked the back of zayn 's head when he was n't concentrating on his math , or the neighborhood lady who would sit on her front stoop knitting hats for each of them so they were a little warmer on those freezing cold , winter days .\", \"they were also generous with their wealth , but that was all done secretly , making sure that the people who had taken care of them as kids were now well cared for , including the cop who had released dominic for stealing some bread one day , or the teacher who had smacked the back of zayn 's head when he was n't concentrating on his math , or the neighborhood lady who would sit on her front stoop knitting hats for each of them so they were a little warmer on those freezing cold , winter days .\", \"they were also generous with their wealth , but that was all done secretly , making sure that the people who had taken care of them as kids were now well cared for , including the cop who had released dominic for stealing some bread one day , or the teacher who had smacked the back of zayn 's head when he was n't concentrating on his math , or the neighborhood lady who would sit on her front stoop knitting hats for each of them so they were a little warmer on those freezing cold , winter days .\", \"third , that girl is desperate to talk about her business with someone , and since she 's coming to me that tells me that either you 're not her fiance and she does n't want to talk to you about anything , or you are her fiance but you have no business sense , which is probably the case because you have n't put a ring on her finger , or you 're a complete fuck-up who does n't care about the thing that 's most important to his future wife .\", \"third , that girl is desperate to talk about her business with someone , and since she 's coming to me that tells me that either you 're not her fiance and she does n't want to talk to you about anything , or you are her fiance but you have no business sense , which is probably the case because you have n't put a ring on her finger , or you 're a complete fuck-up who does n't care about the thing that 's most important to his future wife .\", \"third , that girl is desperate to talk about her business with someone , and since she 's coming to me that tells me that either you 're not her fiance and she does n't want to talk to you about anything , or you are her fiance but you have no business sense , which is probably the case because you have n't put a ring on her finger , or you 're a complete fuck-up who does n't care about the thing that 's most important to his future wife .\", \"third , that girl is desperate to talk about her business with someone , and since she 's coming to me that tells me that either you 're not her fiance and she does n't want to talk to you about anything , or you are her fiance but you have no business sense , which is probably the case because you have n't put a ring on her finger , or you 're a complete fuck-up who does n't care about the thing that 's most important to his future wife .\"], 'text_length': [499, 499, 499, 499, 443, 443, 443, 443]}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4953a123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e808b860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] another man ' s child? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a32df61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128 # BERT is trained with 128 long sequences for the first 90% steps, and then with 512\n",
    "result = tokenize_function(tokenizer, dataset[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c83eaf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2788, 1010, 2002, 2052, 2022, 13311, 2105, 1996, 2542, 2282, 1010, 2652, 2007, 2010, 10899, 1012, 102], [101, 2021, 2074, 2028, 2298, 2012, 1037, 7163, 2239, 2741, 2032, 8134, 4937, 22436, 2594, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'word_ids': [[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, None], [None, 0, 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 10, 10, 11, None]], 'num_tokens': [18, 17]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "411da070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 74004228\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c660d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb1f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c34ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8af29c46",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2788,\n",
       " 1010,\n",
       " 2002,\n",
       " 2052,\n",
       " 2022,\n",
       " 13311,\n",
       " 2105,\n",
       " 1996,\n",
       " 2542,\n",
       " 2282,\n",
       " 1010,\n",
       " 2652,\n",
       " 2007,\n",
       " 2010,\n",
       " 10899,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2021,\n",
       " 2074,\n",
       " 2028,\n",
       " 2298,\n",
       " 2012,\n",
       " 1037,\n",
       " 7163,\n",
       " 2239,\n",
       " 2741,\n",
       " 2032,\n",
       " 8134,\n",
       " 4937,\n",
       " 22436,\n",
       " 2594,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(result[\"input_ids\"], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b38c907",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101,\n",
       "  2788,\n",
       "  1010,\n",
       "  2002,\n",
       "  2052,\n",
       "  2022,\n",
       "  13311,\n",
       "  2105,\n",
       "  1996,\n",
       "  2542,\n",
       "  2282,\n",
       "  1010,\n",
       "  2652,\n",
       "  2007,\n",
       "  2010,\n",
       "  10899,\n",
       "  1012,\n",
       "  102],\n",
       " [101,\n",
       "  2021,\n",
       "  2074,\n",
       "  2028,\n",
       "  2298,\n",
       "  2012,\n",
       "  1037,\n",
       "  7163,\n",
       "  2239,\n",
       "  2741,\n",
       "  2032,\n",
       "  8134,\n",
       "  4937,\n",
       "  22436,\n",
       "  2594,\n",
       "  1012,\n",
       "  102]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b33bbe0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = tokenizer(dataset[0:2][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a06adbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2788, 1010, 2002, 2052, 2022, 13311, 2105, 1996, 2542, 2282, 1010, 2652, 2007, 2010, 10899, 1012, 102], [101, 2021, 2074, 2028, 2298, 2012, 1037, 7163, 2239, 2741, 2032, 8134, 4937, 22436, 2594, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ebb5a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 10, 10, 11, None]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.word_ids(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95332a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] but just one look at a minion sent him practically catatonic. [SEP]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4a17711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'but',\n",
       " 'just',\n",
       " 'one',\n",
       " 'look',\n",
       " 'at',\n",
       " 'a',\n",
       " 'mini',\n",
       " '##on',\n",
       " 'sent',\n",
       " 'him',\n",
       " 'practically',\n",
       " 'cat',\n",
       " '##aton',\n",
       " '##ic',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(result[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61ca4a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['but',\n",
       " 'just',\n",
       " 'one',\n",
       " 'look',\n",
       " 'at',\n",
       " 'a',\n",
       " 'mini',\n",
       " '##on',\n",
       " 'sent',\n",
       " 'him',\n",
       " 'practically',\n",
       " 'cat',\n",
       " '##aton',\n",
       " '##ic',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(dataset[1][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7494da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-from-scratch]",
   "language": "python",
   "name": "conda-env-pytorch-from-scratch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
