{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b53efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from safetensors.torch import load_model\n",
    "from bert.model import BertMLM, BertConfig\n",
    "from bert.data import load_pretraining_dataset, TrainingCollator\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from bert.utils import decode_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581ecf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc35bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig()\n",
    "model = BertMLM(config)\n",
    "weights_before = model.bias.detach().clone()\n",
    "model_save_path = \"/media/bryan/ssd01/expr/bert_from_scratch/debug01/checkpoints/checkpoint_10/model.safetensors\"\n",
    "load_model(model, model_save_path)\n",
    "weight_after = model.bias.detach().clone()\n",
    "assert not torch.allclose(weights_before, weight_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ec1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"/media/bryan/ssd01/expr/bert_from_scratch/debug01/initial_dataset_cache\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=False, collate_fn=TrainingCollator(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a283a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf4983de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    token_logits = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "797ea7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he'd seen the movie almost by mistake, considering he was a little young for the pg cartoon, but with older cousins, along with her brothers, mason was often exposed to things that were older.\n",
      "he [MASK] d seen [MASK] movie almost by mistake, [MASK] he was a little young for the pg cartoon, but with older [MASK], along with her brothers, mason was often exposed to things that were older [MASK]\n",
      "he '[100.0%] d seen a[41.0%] movie almost by mistake, and[64.1%] he was a little young for the pg cartoon, but with older women[28.7%], along with her brothers, mason was often exposed to things that were older.[99.8%] (Top 0 prediction)\n",
      "he.[0.0%] d seen the[30.6%] movie almost by mistake, because[4.9%] he was a little young for the pg cartoon, but with older men[24.4%], along with her brothers, mason was often exposed to things that were older![0.2%] (Top 1 prediction)\n"
     ]
    }
   ],
   "source": [
    "def decode_with_mask(input_ids):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    special_tokens = [x for x in tokenizer.all_special_tokens if x != tokenizer.mask_token]\n",
    "    filtered_tokens = [x for x in tokens if x not in special_tokens]\n",
    "    text = tokenizer.convert_tokens_to_string(filtered_tokens)\n",
    "    clean_text = tokenizer.clean_up_tokenization(text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "batch_original_text = tokenizer.batch_decode(batch[\"original_input_ids\"], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "topk = 2\n",
    "batch_index = 3\n",
    "\n",
    "text = batch_original_text[batch_index]\n",
    "text_with_mask = decode_with_mask(batch[\"input_ids\"][batch_index])\n",
    "\n",
    "mask_token_index = torch.where(batch[\"input_ids\"][batch_index] == tokenizer.mask_token_id)[0]\n",
    "mask_token_logits = token_logits[batch_index, mask_token_index,:]\n",
    "mask_token_probs = torch.softmax(mask_token_logits, dim=1)\n",
    "topk_tokens = torch.topk(mask_token_probs, topk, dim=1)\n",
    "\n",
    "def decode_pred_string(k = 0, with_prob = True):\n",
    "    pred_tokens = tokenizer.convert_ids_to_tokens(batch[\"input_ids\"][batch_index])\n",
    "    for i, token_index in enumerate(mask_token_index.tolist()):\n",
    "        pred_token_id = topk_tokens.indices[i][k].item()\n",
    "        pred_token = tokenizer.convert_ids_to_tokens(pred_token_id)\n",
    "        if with_prob:\n",
    "            pred_prob = topk_tokens.values[i][k].item()\n",
    "            pred_token = f\"{pred_token}[{pred_prob:.1%}]\"\n",
    "        pred_tokens[token_index] = pred_token\n",
    "    filtered_pred_tokens = [x for x in pred_tokens if x not in tokenizer.all_special_tokens]\n",
    "    text = tokenizer.convert_tokens_to_string(filtered_pred_tokens)\n",
    "    clean_text = tokenizer.clean_up_tokenization(text)\n",
    "    return clean_text\n",
    "\n",
    "print(text)\n",
    "print(text_with_mask)\n",
    "for k in range(topk):\n",
    "    pred_text = decode_pred_string(k, with_prob=True)\n",
    "    print(f\"{pred_text} (Top {k} prediction)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "475b0e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = batch[\"input_ids\"][1]\n",
    "torch.count_nonzero(input_ids == tokenizer.mask_token_id).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d38b0a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_batch = decode_batch(tokenizer, batch, token_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5fa0d7ca",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\t\t usually, he would be tearing around the living room, playing with his toys.\n",
      "text_with_mask\t [MASK], he would be tearing around the living room, playing with his toys.\n",
      "pred_top_1\t now[5.8%], he would be tearing around the living room, playing with his toys.\n",
      "pred_top_2\t instead[5.3%], he would be tearing around the living room, playing with his toys.\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "text\t\t but just one look at a minion sent him practically catatonic.\n",
      "text_with_mask\t but just one look at a minion sent [MASK] [MASK] catatonic.\n",
      "pred_top_1\t but just one look at a minion sent him[15.9%] to[14.6%] catatonic.\n",
      "pred_top_2\t but just one look at a minion sent her[14.2%] a[9.1%] catatonic.\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "text\t\t that had been megan's plan when she got him dressed earlier.\n",
      "text_with_mask\t that had been megan'[MASK] plan when [MASK] got him dressed earlier.\n",
      "pred_top_1\t that had been megan's[99.9%] plan when she[40.7%] got him dressed earlier.\n",
      "pred_top_2\t that had been megan'd[0.0%] plan when i[23.0%] got him dressed earlier.\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "text\t\t he'd seen the movie almost by mistake, considering he was a little young for the pg cartoon, but with older cousins, along with her brothers, mason was often exposed to things that were older.\n",
      "text_with_mask\t he'd seen the [MASK] almost by mistake repealed considering he was a little young for the pg cartoon, but with older cousins, [MASK] [MASK] her [MASK], mason was often exposed [MASK] things that were older.\n",
      "pred_top_1\t he'd seen the woman[2.1%] almost by mistake repealed considering he was a little young for the pg cartoon, but with older cousins, and[8.6%] of[11.6%] her father[17.4%], mason was often exposed to[41.0%] things that were older.\n",
      "pred_top_2\t he'd seen the man[2.0%] almost by mistake repealed considering he was a little young for the pg cartoon, but with older cousins, all[6.7%] to[10.1%] her husband[7.2%], mason was often exposed by[39.5%] things that were older.\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "text\t\t she liked to think being surrounded by adults and older kids was one reason why he was a such a good talker for his age.\n",
      "text_with_mask\t [MASK] liked to think being surrounded by adults [MASK] older kids was one [MASK] [MASK] he was a such a good talker for his age.\n",
      "pred_top_1\t he[51.8%] liked to think being surrounded by adults and[18.1%] older kids was one,[20.5%] but[37.2%] he was a such a good talker for his age.\n",
      "pred_top_2\t she[18.4%] liked to think being surrounded by adults,[14.8%] older kids was one thing[17.4%] and[20.5%] he was a such a good talker for his age.\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "text\t\t mason barely acknowledged her.\n",
      "text_with_mask\t mason barely acknowledged [MASK] [MASK]\n",
      "pred_top_1\t mason barely acknowledged her[37.2%].[99.7%]\n",
      "pred_top_2\t mason barely acknowledged him[19.0%]![0.2%]\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
      "text\t\t since the movie was almost over, megan knew she better slip into the bedroom and finish getting ready.\n",
      "text_with_mask\t [MASK] the movie was almost over [MASK] megan knew she better [MASK] into the bedroom and finish getting ready.\n",
      "pred_top_1\t when[46.7%] the movie was almost over,[85.5%] megan knew she better get[48.5%] into the bedroom and finish getting ready.\n",
      "pred_top_2\t once[12.5%] the movie was almost over and[9.2%] megan knew she better go[11.3%] into the bedroom and finish getting ready.\n",
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n"
     ]
    }
   ],
   "source": [
    "for decode in decoded_batch:\n",
    "    for k,v in decode.items():\n",
    "        tabs = \"\\t\\t\" if k == \"text\" else \"\\t\"\n",
    "        print(f\"{k}{tabs}\", v)\n",
    "    print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef76337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import datasets\n",
    "\n",
    "class PerplexityForMLM(evaluate.Metric):\n",
    "    def _info(self):\n",
    "        return evaluate.MetricInfo(\n",
    "            module_type=\"metric\",\n",
    "            description=\"Perplexity for masked language models\",\n",
    "            citation=\"\",\n",
    "            inputs_description= \"Logits and true token IDs\",\n",
    "            features = datasets.Features(\n",
    "            {\n",
    "                \"logits\": datasets.Value(\"float32\"),\n",
    "                \"references\": datasets.Value(\"int32\"),\n",
    "            }),\n",
    "        )\n",
    "\n",
    "    def _compute(self, logits, references):\n",
    "        log_probs = torch.log_softmax(torch.tensor(logits), dim=-1)\n",
    "        nll = -log_probs[range(len(references)), references]\n",
    "        avg_nll = nll.mean()\n",
    "        return {\"perplexity\": torch.exp(avg_nll).item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a32e1a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_batch_indices, mask_token_seq_indices = torch.where(batch[\"input_ids\"] == tokenizer.mask_token_id)\n",
    "mask_token_logits = token_logits[mask_token_batch_indices, mask_token_seq_indices, :]\n",
    "true_token_ids = batch[\"labels\"][mask_token_batch_indices, mask_token_seq_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e094407",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Module inputs don't match the expected format.\nExpected format: {'logits': Value(dtype='float32', id=None), 'references': Value(dtype='int32', id=None)},\nInput logits: [[ -9.368389   -4.121412   -4.546527  ...  -4.754039   -4.011625\n   -4.8967376]\n [ -5.4772935  -1.5144248  -2.090969  ...  -3.2176354  -1.409075\n   -1.9109528]\n [-10.223923   -4.045441   -4.095291  ...  -4.6059313  -4.705429\n   -4.9013143]\n ...\n [ -9.298979   -4.258144   -4.248714  ...  -4.0665255  -4.6621933\n   -4.9004464]\n [-10.275549   -6.0642056  -4.7409644 ...  -4.5790854  -4.271665\n   -5.9925475]\n [-13.01876    -5.8997355  -6.4239573 ...  -5.3757486  -6.581865\n   -6.3337   ]],\nInput references: [13311 12756  2933  1005  1037  2021  1010  2411  2000  2108  2001  1037\n  2287  1036  1029  4510  2471  2046]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m perplexity_metric \u001b[38;5;241m=\u001b[39m PerplexityForMLM()\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m perplexity_metric\u001b[38;5;241m.\u001b[39mcompute(logits\u001b[38;5;241m=\u001b[39mmask_token_logits\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), references\u001b[38;5;241m=\u001b[39mtrue_token_ids\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom Metric Perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperplexity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-from-scratch/lib/python3.12/site-packages/evaluate/module.py:455\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-from-scratch/lib/python3.12/site-packages/evaluate/module.py:546\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m     )\n\u001b[0;32m--> 546\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Module inputs don't match the expected format.\nExpected format: {'logits': Value(dtype='float32', id=None), 'references': Value(dtype='int32', id=None)},\nInput logits: [[ -9.368389   -4.121412   -4.546527  ...  -4.754039   -4.011625\n   -4.8967376]\n [ -5.4772935  -1.5144248  -2.090969  ...  -3.2176354  -1.409075\n   -1.9109528]\n [-10.223923   -4.045441   -4.095291  ...  -4.6059313  -4.705429\n   -4.9013143]\n ...\n [ -9.298979   -4.258144   -4.248714  ...  -4.0665255  -4.6621933\n   -4.9004464]\n [-10.275549   -6.0642056  -4.7409644 ...  -4.5790854  -4.271665\n   -5.9925475]\n [-13.01876    -5.8997355  -6.4239573 ...  -5.3757486  -6.581865\n   -6.3337   ]],\nInput references: [13311 12756  2933  1005  1037  2021  1010  2411  2000  2108  2001  1037\n  2287  1036  1029  4510  2471  2046]"
     ]
    }
   ],
   "source": [
    "perplexity_metric = PerplexityForMLM()\n",
    "result = perplexity_metric.compute(logits=mask_token_logits.cpu().numpy(), references=true_token_ids.cpu().numpy())\n",
    "print(f\"Custom Metric Perplexity: {result['perplexity']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-from-scratch]",
   "language": "python",
   "name": "conda-env-pytorch-from-scratch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
